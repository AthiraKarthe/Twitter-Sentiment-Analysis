# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rv22W6sN80auLACAsAUB_KsmLjz_Evxv
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# train_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')
# test_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')
train_df = pd.read_csv('C:/Users/Dell/Downloads/train.csv')
test_df = pd.read_csv('C:/Users/Dell/Downloads/test (1).csv')

train_text = list(train_df.text)
train_sentiment = list(train_df.sentiment)
train_selected = list(train_df.selected_text)
test_text = list(test_df.text)
test_sentiment = list(test_df.sentiment)

# !pip install tensorflow

import re
test_curated_text = []
train_curated_text = []
for text in train_text:
    train_curated_text.append( re.sub(r"http\S+", "", str(text)))
for text in test_text:
    test_curated_text.append(re.sub(r'http\S+',"",str(text)))

from tensorflow.keras.preprocessing.text import Tokenizer
clean_text = []
tokenizer = Tokenizer(50000)
tokenizer.fit_on_texts(train_curated_text)

word_index = tokenizer.word_index

max_length = max([len(text) for text in train_curated_text])
from tensorflow.keras.preprocessing.sequence import pad_sequences
sequences = tokenizer.texts_to_sequences(train_curated_text)
padded = pad_sequences(sequences,maxlen=max_length, truncating='post')

testing_sequences = tokenizer.texts_to_sequences(test_curated_text)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length)

vocab_size = len(word_index)
embedding_dim = 16

count = 0
for x in test_sentiment:
    if x!='positive'and x!="negative" and x!='neutral':
        count+=1

from sklearn import preprocessing
lb = preprocessing.LabelBinarizer()
lb.fit(train_sentiment)

training_labels_final = lb.transform(train_sentiment)
testing_labels_final = lb.transform(test_sentiment)

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length),
    tf.keras.layers.GlobalAvgPool1D(),
    # tf.keras.layers.GRU(6),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

num_epochs = 10
history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))

# Visualising metrics Training vs Validation
from matplotlib import pyplot as plt
plt.plot(history.history['val_acc'])
plt.plot(history.history['acc'])
plt.title('model accuracy')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()

example = tokenizer.texts_to_sequences(["i have work to do"])
example = pad_sequences(example,maxlen=max_length)
# example = tf.convert_to_tensor(example)
pred = model.predict(example)
print(pred[0])

import tweepy as tw
import pandas as pd
consumer_key= 'kMW0xqTbbK0OaASX8hWj4vSmM'
consumer_secret= 'nnBerO8NkPBVaPXM3xkntuAV0ZXIeksyv0V0bnOA6JKtL1vMgE'
access_token= '1275779687323590656-YGIfDnzXVTsR1SfFIbIJUdgENMkcb2'
access_token_secret= 'MkCPPmOURytI4clu0RmrPagcdcjsRznqQx3y12gUwJhKB'
auth = tw.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tw.API(auth, wait_on_rate_limit=True)

search_words = "#hulk"
date_since = "2000-11-16"
tweets = tw.Cursor(api.search,
              q=search_words,
              lang="en",
              since=date_since).items(50000)

listoftweet = [tweet.text for tweet in tweets]


def clean_text(list_of_tweets):
  count = 0
  clean_texts = []
  # for tweet in list_of_tweets:
    # link_removed.append( re.sub(r"http\S+", "", str(tweet)))
  for tweet in list_of_tweets:
    count+=1
    clean_texts.append(re.sub(r"(\n)|(#\S+)|(@\S+)|(http\S+)","",str(tweet)))
  return count,clean_texts

c,ct = clean_text(listoftweet)

rt_removed = []
for tweet in ct:
  if not (re.match('^(RT)',tweet)):
    rt_removed.append(tweet)

text_sequences = tokenizer.texts_to_sequences(rt_removed)
text_padded = pad_sequences(text_sequences,maxlen=max_length,truncating='post')
preds = model.predict(text_padded)


indices = []
for pred in preds:
  indices.append(np.argmax(pred))


neutral = indices.count(1)
positive = indices.count(2)
negative = indices.count(0)
print('neutral-',neutral,' negative-',negative,' positive-',positive)

from tkinter import *
from tkinter import ttk
root = Tk()
negativeprogress = ttk.Progressbar(root, orient = HORIZONTAL, length = 120)
neutralprogress = ttk.Progressbar(root, orient = HORIZONTAL, length = 120)
positiveprogress = ttk.Progressbar(root, orient = HORIZONTAL, length = 120)
negativeprogress.pack()
neutralprogress.pack()
positiveprogress.pack()
negativeprogress.config(mode = 'determinate',maximum=100, value =negative )
neutralprogress.config(mode = 'determinate',maximum=100, value =neutral )
positiveprogress.config(mode = 'determinate',maximum=100, value =positive )
negativeprogress.step(5)
neutralprogress.step(5)
positiveprogress.step(5)
negativeprogress.config(mode='indeterminate')
positiveprogress.config(mode='indeterminate')
neutralprogress.config(mode='indeterminate')
positiveprogress.start()
negativeprogress.stop()
neutralprogress.stop()
positiveprogress.stop()
root.mainloop()
